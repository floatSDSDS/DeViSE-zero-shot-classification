{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from random import *\n",
    "import scipy.io as sio\n",
    "import input_data\n",
    "import model_torch as model\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tool\n",
    "import math\n",
    "from fastai.conv_learner import *\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "a = Random()\n",
    "a.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_description='testDeVise'\n",
    "rep_num = 1\n",
    "id_split=range(0,10)\n",
    "# SNIP, SMP18\n",
    "choose_dataset=\"SNIP\"\n",
    "# Caps, CapsDim,CapsWS, CapsAll\n",
    "choose_model=[]\n",
    "# without seen: 0, with seen: 1, fixed with some classes: -1\n",
    "dataSetting={}\n",
    "dataSetting['test_mode']=0\n",
    "######\n",
    "dataSetting['random_class']=False\n",
    "dataSetting['training_prob']=0.8\n",
    "dataSetting['test_intrain_prob']=0.3\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "dataSetting['data_prefix']='data/SNIP/'\n",
    "dataSetting['dataset_name']='dataSNIP.txt'\n",
    "dataSetting['wordvec_name']='wiki.en.vec'\n",
    "dataSetting['sim_name_withS']='SNIP_similarity_M_zscore.mat'\n",
    "dataSetting['sim_name_withOS']='SNIP10seen.mat'\n",
    "if choose_dataset==\"SMP18\":\n",
    "    dataSetting['data_prefix']='data/SMP18/'\n",
    "    dataSetting['dataset_name']='dataSMP18.txt'\n",
    "    dataSetting['wordvec_name']='sgns_merge_subsetSMP.txt'\n",
    "    dataSetting['sim_name_withS']='SMP_similarity_M_zscore.mat'\n",
    "    dataSetting['sim_name_withOS']='SMP44_wSeen_R1.mat'\n",
    "\n",
    "if choose_dataset =='SNIP':\n",
    "    dataSetting['unseen_class'] = [['playlist'], ['book']]\n",
    "elif choose_dataset =='SMP18':\n",
    "    dataSetting['unseen_class'] = [['聊天'],['网站'],['email'],['地图'],['时间'],['健康']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setting(data):\n",
    "    vocab_size, word_emb_size = data['embedding'].shape\n",
    "    sample_num, max_time = data['x_tr'].shape\n",
    "    test_num = data['x_te'].shape[0]\n",
    "    s_cnum = np.unique(data['y_tr']).shape[0]\n",
    "    u_cnum = np.unique(data['y_te']).shape[0]\n",
    "    config = {}\n",
    "    config['model_name'] = choose_model\n",
    "    config['dataset']=choose_dataset\n",
    "    config['test_mode']=dataSetting['test_mode']\n",
    "    config['training_prob']=dataSetting['training_prob']\n",
    "    config['test_intrain_prob']=dataSetting['test_intrain_prob']\n",
    "    config['wordvec']=dataSetting['wordvec_name']\n",
    "    config['sim_name_withS']=dataSetting['sim_name_withS']\n",
    "    config['sim_name_withOS']=dataSetting['sim_name_withOS']\n",
    "    config['keep_prob'] = 0.5 # embedding dropout keep rate\n",
    "    config['hidden_size'] = 16 # embedding vector size\n",
    "    config['batch_size'] = 50 # vocab size of word vectors\n",
    "    config['vocab_size'] = vocab_size # vocab size of word vectors (10,895)\n",
    "    config['num_epochs'] = 20 # number of epochs\n",
    "    config['max_time'] = max_time\n",
    "    config['sample_num'] = sample_num #sample number of training data\n",
    "    config['test_num'] = test_num #number of test data\n",
    "    config['s_cnum'] = s_cnum # seen class num\n",
    "    config['u_cnum'] = u_cnum #unseen class num\n",
    "    config['word_emb_size'] = word_emb_size # embedding size of word vectors (300)\n",
    "    config['d_a'] = 10 # self-attention weight hidden units number\n",
    "    config['output_atoms'] = 10 #capsule output atoms\n",
    "    config['r'] = 3 #self-attention weight hops\n",
    "    config['num_routing'] = 3 #capsule routing num\n",
    "    config['alpha'] = 0.001 # coefficient of self-attention loss\n",
    "    config['margin'] = 1.0 # ranking loss margin\n",
    "    config['learning_rate'] = 0.1\n",
    "    config['lr_step_size']=10\n",
    "    config['lr_gamma']=0.1\n",
    "    config['sim_scale'] = 4 #sim scale\n",
    "    config['nlayers'] = 2 # default for bilstm\n",
    "    config['seen_class']=data['seen_class']\n",
    "    config['unseen_class']=data['unseen_class']\n",
    "    config['data_prefix']=dataSetting['data_prefix']\n",
    "    config['ckpt_dir'] = './'+test_description+'/' #check point dir\n",
    "    config['experiment_time']= time.strftime('%y%m%d%I%M%S')\n",
    "    config['best_epoch']=0\n",
    "    config['best_acc']=0\n",
    "    config['report']=True\n",
    "    config['cuda_id']=0\n",
    "    config['untrain_classlen']=data['untrain_classlen']#XIAOTONG\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(n, batch_size):\n",
    "    batch_index = a.sample(range(n), batch_size)\n",
    "    return batch_index\n",
    "\n",
    "def sort_batch(batch_x, batch_y, batch_len, batch_ind):\n",
    "    batch_len_new = batch_len\n",
    "    batch_len_new, perm_idx = batch_len_new.sort(0, descending=True)\n",
    "    batch_x_new = batch_x[perm_idx]\n",
    "    batch_y_new = batch_y[perm_idx]\n",
    "    batch_ind_new = batch_ind[perm_idx]\n",
    "\n",
    "    return batch_x_new, batch_y_new, \\\n",
    "           batch_len_new, batch_ind_new\n",
    "\n",
    "def cos_loss(input, target):\n",
    "    return 1 - F.cosine_similarity(input, target).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myLSTM(nn.Module):\n",
    "    def __init__(self, config, pretrained_embedding = None):\n",
    "        super(myLSTM, self).__init__()\n",
    "        \n",
    "        self.cuda_id=config['cuda_id']\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        self.vocab_size = config['vocab_size']\n",
    "        self.word_emb_size = config['word_emb_size']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.s_cnum = config['s_cnum']\n",
    "        self.nlayers = config['nlayers']\n",
    "        \n",
    "\n",
    "        self.word_embedding = nn.Embedding(config['vocab_size'], config['word_emb_size'])\n",
    "        self.bilstm = nn.LSTM(config['word_emb_size'], config['hidden_size'],\n",
    "                              config['nlayers'], bidirectional=True, batch_first=True)\n",
    "        self.drop = nn.Dropout(config['keep_prob'])\n",
    "        self.predict =nn.Linear(config['hidden_size']*2,config['s_cnum'])\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input,len, embedding):\n",
    "        self.s_len = len\n",
    "        input = input.transpose(0,1) #(Bach,Length,D) => (L,B,D)\n",
    "        # Attention\n",
    "        if (embedding.nelement() != 0):\n",
    "            self.word_embedding = nn.Embedding.from_pretrained(embedding)\n",
    "\n",
    "        emb = self.word_embedding(input)\n",
    "        packed_emb = pack_padded_sequence(emb, len)\n",
    "\n",
    "        #Initialize hidden states\n",
    "        h_0 = torch.zeros(self.nlayers*2, input.shape[1], self.hidden_size)\n",
    "        c_0 = torch.zeros(self.nlayers*2, input.shape[1], self.hidden_size)\n",
    "        if torch.cuda.is_available():\n",
    "            h_0=h_0.cuda(self.cuda_id)\n",
    "            c_0=c_0.cuda(self.cuda_id)\n",
    "        \n",
    "        outp, (final_hidden_state,final_cell_state) = self.bilstm(packed_emb, (h_0, c_0))## [bsz, len, d_h * 2]\n",
    "        self.hh=torch.cat((final_hidden_state[-1],final_hidden_state[-2]),1)\n",
    "        final_output=self.predict(self.hh)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------read datasets begin-------------------\n",
      "------------------load word2vec begin-------------------\n",
      "------------------load word2vec end---------------------\n",
      "------------------read datasets end---------------------\n",
      "------------------use gpu------------------\n",
      "------------------training begin---------------------\n",
      "sample_number= 9881  ,batch_size= 50 batch_num= 198\n"
     ]
    }
   ],
   "source": [
    "    data = input_data.read_datasets(dataSetting)\n",
    "    \n",
    "    # load settings\n",
    "    config = setting(data)\n",
    "    cuda_id=config['cuda_id']\n",
    "            \n",
    "    x_tr = torch.from_numpy(data['x_tr'])\n",
    "    y_tr = torch.from_numpy(data['y_tr'])\n",
    "    y_tr_id = torch.from_numpy(data['y_tr'])\n",
    "    y_te_id = torch.from_numpy(data['y_te'])\n",
    "    y_ind = torch.from_numpy(data['s_label'])\n",
    "    s_len = torch.from_numpy(data['s_len'])\n",
    "    embedding = torch.from_numpy(data['embedding'])\n",
    "    x_te = torch.from_numpy(data['x_te'])\n",
    "    u_len = torch.from_numpy(data['u_len'])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        x_tr =x_tr.cuda(cuda_id)\n",
    "        y_tr =y_tr.cuda(cuda_id)\n",
    "        y_tr_id = y_tr_id.cuda(cuda_id)\n",
    "        y_te_id =y_te_id.cuda(cuda_id)\n",
    "        y_ind =y_ind.cuda(cuda_id)\n",
    "        s_len = s_len.cuda(cuda_id)\n",
    "        embedding=embedding.cuda(cuda_id)\n",
    "        x_te = x_te.cuda(cuda_id)\n",
    "        u_len = u_len.cuda(cuda_id)\n",
    "        print('------------------use gpu------------------')\n",
    "    \n",
    "# Training cycle\n",
    "    batch_num = int(config['sample_num'] / config['batch_size']+1)\n",
    "\n",
    "    # load model\n",
    "    lstm=model.myLSTM(config,embedding).cuda()\n",
    "    loss_fn = F.cross_entropy\n",
    "    optimizer = optim.Adam(lstm.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    if not os.path.exists(config['ckpt_dir']):\n",
    "        os.mkdir(config['ckpt_dir'])\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduce=False, size_average=False)\n",
    "    hh=np.zeros((config['sample_num'],config['hidden_size']*2))\n",
    "\n",
    "    print('------------------training begin---------------------')\n",
    "    print('sample_number=',config['sample_num'],\" ,batch_size=\",config['batch_size'],'batch_num=',batch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 1\n",
    "import torch._utils\n",
    "try:\n",
    "    torch._utils._rebuild_tensor_v2\n",
    "except AttributeError:\n",
    "    def _rebuild_tensor_v2(storage, storage_offset, size, stride, requires_grad, backward_hooks):\n",
    "        tensor = torch._utils._rebuild_tensor(storage, storage_offset, size, stride)\n",
    "        tensor.requires_grad = requires_grad\n",
    "        tensor._backward_hooks = backward_hooks\n",
    "        return tensor\n",
    "    torch._utils._rebuild_tensor_v2 = _rebuild_tensor_v2\n",
    "myModel=torch.load(choose_dataset+'_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-4fb931cd44c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Zeroshot Learning part\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmyModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "# Zeroshot Learning part\n",
    "p=0.1\n",
    "for param in myModel.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modeldata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-991221d32a3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                      \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                      nn.Linear(in_features=512, out_features=300, bias=True))\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlearn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLearner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodeldata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSingleModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_gpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.99\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'modeldata' is not defined"
     ]
    }
   ],
   "source": [
    "myModel.fc = nn.Sequential(nn.BatchNorm1d(512),\n",
    "                     nn.Dropout(p),\n",
    "                     nn.Linear(in_features=512, out_features=512, bias=True),\n",
    "                     nn.ReLU(),\n",
    "                     nn.BatchNorm1d(512),\n",
    "                     nn.Dropout(p),\n",
    "                     nn.Linear(in_features=512, out_features=300, bias=True))\n",
    "learn = Learner(modeldata, SingleModel(to_gpu(model)))\n",
    "learn.opt_fn = partial(optim.Adam, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
